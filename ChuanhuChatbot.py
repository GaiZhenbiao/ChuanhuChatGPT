# -*- coding:utf-8 -*-
import os
import logging
import sys

import gradio as gr

from modules import config
from modules.config import *
from modules.utils import *
from modules.presets import *
from modules.overwrites import *
from modules.models import ModelManager

gr.Chatbot.postprocess = postprocess
PromptHelper.compact_text_chunks = compact_text_chunks

with open("assets/custom.css", "r", encoding="utf-8") as f:
    customCSS = f.read()

with gr.Blocks(css=customCSS, theme=small_and_beautiful_theme) as demo:
    user_name = gr.State("")
    promptTemplates = gr.State(load_template(get_template_names(plain=True)[0], mode=2))
    user_question = gr.State("")
    current_model = gr.State(ModelManager(model_name = MODELS[DEFAULT_MODEL], access_key = my_api_key))

    topic = gr.State("æœªå‘½åå¯¹è¯å†å²è®°å½•")

    with gr.Row():
        gr.HTML(CHUANHU_TITLE, elem_id="app_title")
        status_display = gr.Markdown(get_geoip(), elem_id="status_display")
    with gr.Row(elem_id="float_display"):
        user_info = gr.Markdown(value="getting user info...", elem_id="user_info")

        # https://github.com/gradio-app/gradio/pull/3296
        def create_greeting(request: gr.Request):
            if hasattr(request, "username") and request.username: # is not None or is not ""
                logging.info(f"Get User Name: {request.username}")
                return gr.Markdown.update(value=f"User: {request.username}"), request.username
            else:
                return gr.Markdown.update(value=f"User: default", visible=False), ""
        demo.load(create_greeting, inputs=None, outputs=[user_info, user_name])

    with gr.Row().style(equal_height=True):
        with gr.Column(scale=5):
            with gr.Row():
                chatbot = gr.Chatbot(elem_id="chuanhu_chatbot").style(height="100%")
            with gr.Row():
                with gr.Column(min_width=225, scale=12):
                    user_input = gr.Textbox(
                        elem_id="user_input_tb",
                        show_label=False, placeholder="åœ¨è¿™é‡Œè¾“å…¥"
                    ).style(container=False)
                with gr.Column(min_width=42, scale=1):
                    submitBtn = gr.Button(value="", variant="primary", elem_id="submit_btn")
                    cancelBtn = gr.Button(value="", variant="secondary", visible=False, elem_id="cancel_btn")
            with gr.Row():
                emptyBtn = gr.Button(
                    "ğŸ§¹ æ–°çš„å¯¹è¯",
                )
                retryBtn = gr.Button("ğŸ”„ é‡æ–°ç”Ÿæˆ")
                delFirstBtn = gr.Button("ğŸ—‘ï¸ åˆ é™¤æœ€æ—§å¯¹è¯")
                delLastBtn = gr.Button("ğŸ—‘ï¸ åˆ é™¤æœ€æ–°å¯¹è¯")

        with gr.Column():
            with gr.Column(min_width=50, scale=1):
                with gr.Tab(label="æ¨¡å‹"):
                    keyTxt = gr.Textbox(
                        show_label=True,
                        placeholder=f"OpenAI API-key...",
                        value=hide_middle_chars(my_api_key),
                        type="password",
                        visible=not HIDE_MY_KEY,
                        label="API-Key",
                    )
                    if multi_api_key:
                        usageTxt = gr.Markdown("å¤šè´¦å·æ¨¡å¼å·²å¼€å¯ï¼Œæ— éœ€è¾“å…¥keyï¼Œå¯ç›´æ¥å¼€å§‹å¯¹è¯", elem_id="usage_display", elem_classes="insert_block")
                    else:
                        usageTxt = gr.Markdown("**å‘é€æ¶ˆæ¯** æˆ– **æäº¤key** ä»¥æ˜¾ç¤ºé¢åº¦", elem_id="usage_display", elem_classes="insert_block")
                    model_select_dropdown = gr.Dropdown(
                        label="é€‰æ‹©æ¨¡å‹", choices=MODELS, multiselect=False, value=MODELS[DEFAULT_MODEL], interactive=True
                    )
                    lora_select_dropdown = gr.Dropdown(
                        label="é€‰æ‹©LoRAæ¨¡å‹", choices=[], multiselect=False, interactive=True, visible=False
                    )
                    use_streaming_checkbox = gr.Checkbox(
                        label="å®æ—¶ä¼ è¾“å›ç­”", value=True, visible=ENABLE_STREAMING_OPTION
                    )
                    use_websearch_checkbox = gr.Checkbox(label="ä½¿ç”¨åœ¨çº¿æœç´¢", value=False)
                    language_select_dropdown = gr.Dropdown(
                        label="é€‰æ‹©å›å¤è¯­è¨€ï¼ˆé’ˆå¯¹æœç´¢&ç´¢å¼•åŠŸèƒ½ï¼‰",
                        choices=REPLY_LANGUAGES,
                        multiselect=False,
                        value=REPLY_LANGUAGES[0],
                    )
                    index_files = gr.Files(label="ä¸Šä¼ ç´¢å¼•æ–‡ä»¶", type="file")
                    two_column = gr.Checkbox(label="åŒæ pdf", value=advance_docs["pdf"].get("two_column", False))
                    # TODO: å…¬å¼ocr
                    # formula_ocr = gr.Checkbox(label="è¯†åˆ«å…¬å¼", value=advance_docs["pdf"].get("formula_ocr", False))

                with gr.Tab(label="Prompt"):
                    systemPromptTxt = gr.Textbox(
                        show_label=True,
                        placeholder=f"åœ¨è¿™é‡Œè¾“å…¥System Prompt...",
                        label="System prompt",
                        value=INITIAL_SYSTEM_PROMPT,
                        lines=10,
                    ).style(container=False)
                    with gr.Accordion(label="åŠ è½½Promptæ¨¡æ¿", open=True):
                        with gr.Column():
                            with gr.Row():
                                with gr.Column(scale=6):
                                    templateFileSelectDropdown = gr.Dropdown(
                                        label="é€‰æ‹©Promptæ¨¡æ¿é›†åˆæ–‡ä»¶",
                                        choices=get_template_names(plain=True),
                                        multiselect=False,
                                        value=get_template_names(plain=True)[0],
                                    ).style(container=False)
                                with gr.Column(scale=1):
                                    templateRefreshBtn = gr.Button("ğŸ”„ åˆ·æ–°")
                            with gr.Row():
                                with gr.Column():
                                    templateSelectDropdown = gr.Dropdown(
                                        label="ä»Promptæ¨¡æ¿ä¸­åŠ è½½",
                                        choices=load_template(
                                            get_template_names(plain=True)[0], mode=1
                                        ),
                                        multiselect=False,
                                    ).style(container=False)

                with gr.Tab(label="ä¿å­˜/åŠ è½½"):
                    with gr.Accordion(label="ä¿å­˜/åŠ è½½å¯¹è¯å†å²è®°å½•", open=True):
                        with gr.Column():
                            with gr.Row():
                                with gr.Column(scale=6):
                                    historyFileSelectDropdown = gr.Dropdown(
                                        label="ä»åˆ—è¡¨ä¸­åŠ è½½å¯¹è¯",
                                        choices=get_history_names(plain=True),
                                        multiselect=False,
                                        value=get_history_names(plain=True)[0],
                                    )
                                with gr.Column(scale=1):
                                    historyRefreshBtn = gr.Button("ğŸ”„ åˆ·æ–°")
                            with gr.Row():
                                with gr.Column(scale=6):
                                    saveFileName = gr.Textbox(
                                        show_label=True,
                                        placeholder=f"è®¾ç½®æ–‡ä»¶å: é»˜è®¤ä¸º.jsonï¼Œå¯é€‰ä¸º.md",
                                        label="è®¾ç½®ä¿å­˜æ–‡ä»¶å",
                                        value="å¯¹è¯å†å²è®°å½•",
                                    ).style(container=True)
                                with gr.Column(scale=1):
                                    saveHistoryBtn = gr.Button("ğŸ’¾ ä¿å­˜å¯¹è¯")
                                    exportMarkdownBtn = gr.Button("ğŸ“ å¯¼å‡ºä¸ºMarkdown")
                                    gr.Markdown("é»˜è®¤ä¿å­˜äºhistoryæ–‡ä»¶å¤¹")
                            with gr.Row():
                                with gr.Column():
                                    downloadFile = gr.File(interactive=True)

                with gr.Tab(label="é«˜çº§"):
                    gr.Markdown("# âš ï¸ åŠ¡å¿…è°¨æ…æ›´æ”¹ âš ï¸\n\nå¦‚æœæ— æ³•ä½¿ç”¨è¯·æ¢å¤é»˜è®¤è®¾ç½®")
                    gr.HTML(APPEARANCE_SWITCHER, elem_classes="insert_block")
                    with gr.Accordion("å‚æ•°", open=False):
                        temperature_slider = gr.Slider(
                            minimum=-0,
                            maximum=2.0,
                            value=1.0,
                            step=0.1,
                            interactive=True,
                            label="temperature",
                        )
                        top_p_slider = gr.Slider(
                            minimum=-0,
                            maximum=1.0,
                            value=1.0,
                            step=0.05,
                            interactive=True,
                            label="top-p",
                        )
                        n_choices_slider = gr.Slider(
                            minimum=1,
                            maximum=10,
                            value=1,
                            step=1,
                            interactive=True,
                            label="n choices",
                        )
                        stop_sequence_txt = gr.Textbox(
                            show_label=True,
                            placeholder=f"åœ¨è¿™é‡Œè¾“å…¥åœæ­¢ç¬¦ï¼Œç”¨è‹±æ–‡é€—å·éš”å¼€...",
                            label="stop",
                            value="",
                            lines=1,
                        )
                        max_context_length_slider = gr.Slider(
                            minimum=1,
                            maximum=32768,
                            value=2000,
                            step=1,
                            interactive=True,
                            label="max context",
                        )
                        max_generation_slider = gr.Slider(
                            minimum=1,
                            maximum=32768,
                            value=1000,
                            step=1,
                            interactive=True,
                            label="max generations",
                        )
                        presence_penalty_slider = gr.Slider(
                            minimum=-2.0,
                            maximum=2.0,
                            value=0.0,
                            step=0.01,
                            interactive=True,
                            label="presence penalty",
                        )
                        frequency_penalty_slider = gr.Slider(
                            minimum=-2.0,
                            maximum=2.0,
                            value=0.0,
                            step=0.01,
                            interactive=True,
                            label="frequency penalty",
                        )
                        logit_bias_txt = gr.Textbox(
                            show_label=True,
                            placeholder=f"word:likelihood",
                            label="logit bias",
                            value="",
                            lines=1,
                        )
                        user_identifier_txt = gr.Textbox(
                            show_label=True,
                            placeholder=f"ç”¨äºå®šä½æ»¥ç”¨è¡Œä¸º",
                            label="ç”¨æˆ·å",
                            value=user_name.value,
                            lines=1,
                        )

                    with gr.Accordion("ç½‘ç»œè®¾ç½®", open=False):
                        # ä¼˜å…ˆå±•ç¤ºè‡ªå®šä¹‰çš„api_host
                        apihostTxt = gr.Textbox(
                            show_label=True,
                            placeholder=f"åœ¨è¿™é‡Œè¾“å…¥API-Host...",
                            label="API-Host",
                            value=config.api_host or shared.API_HOST,
                            lines=1,
                        )
                        changeAPIURLBtn = gr.Button("ğŸ”„ åˆ‡æ¢APIåœ°å€")
                        proxyTxt = gr.Textbox(
                            show_label=True,
                            placeholder=f"åœ¨è¿™é‡Œè¾“å…¥ä»£ç†åœ°å€...",
                            label="ä»£ç†åœ°å€ï¼ˆç¤ºä¾‹ï¼šhttp://127.0.0.1:10809ï¼‰",
                            value="",
                            lines=2,
                        )
                        changeProxyBtn = gr.Button("ğŸ”„ è®¾ç½®ä»£ç†åœ°å€")
                        default_btn = gr.Button("ğŸ”™ æ¢å¤é»˜è®¤è®¾ç½®")

    gr.Markdown(CHUANHU_DESCRIPTION)
    gr.HTML(FOOTER.format(versions=versions_html()), elem_id="footer")
    chatgpt_predict_args = dict(
        fn=current_model.value.predict,
        inputs=[
            user_question,
            chatbot,
            use_streaming_checkbox,
            use_websearch_checkbox,
            index_files,
            language_select_dropdown,
        ],
        outputs=[chatbot, status_display],
        show_progress=True,
    )

    start_outputing_args = dict(
        fn=start_outputing,
        inputs=[],
        outputs=[submitBtn, cancelBtn],
        show_progress=True,
    )

    end_outputing_args = dict(
        fn=end_outputing, inputs=[], outputs=[submitBtn, cancelBtn]
    )

    reset_textbox_args = dict(
        fn=reset_textbox, inputs=[], outputs=[user_input]
    )

    transfer_input_args = dict(
        fn=transfer_input, inputs=[user_input], outputs=[user_question, user_input, submitBtn, cancelBtn], show_progress=True
    )

    get_usage_args = dict(
        fn=current_model.value.billing_info, inputs=None, outputs=[usageTxt], show_progress=False
    )

    load_history_from_file_args = dict(
        fn=current_model.value.load_chat_history,
        inputs=[historyFileSelectDropdown, chatbot, user_name],
        outputs=[saveFileName, systemPromptTxt, chatbot]
    )


    # Chatbot
    cancelBtn.click(current_model.value.interrupt, [], [])

    user_input.submit(**transfer_input_args).then(**chatgpt_predict_args).then(**end_outputing_args)
    user_input.submit(**get_usage_args)

    submitBtn.click(**transfer_input_args).then(**chatgpt_predict_args).then(**end_outputing_args)
    submitBtn.click(**get_usage_args)

    emptyBtn.click(
        current_model.value.reset,
        outputs=[chatbot, status_display],
        show_progress=True,
    )
    emptyBtn.click(**reset_textbox_args)

    retryBtn.click(**start_outputing_args).then(
        current_model.value.retry,
        [
            chatbot,
            use_streaming_checkbox,
            use_websearch_checkbox,
            index_files,
            language_select_dropdown,
        ],
        [chatbot, status_display],
        show_progress=True,
    ).then(**end_outputing_args)
    retryBtn.click(**get_usage_args)

    delFirstBtn.click(
        current_model.value.delete_first_conversation,
        None,
        [status_display],
    )

    delLastBtn.click(
        current_model.value.delete_last_conversation,
        [chatbot],
        [chatbot, status_display],
        show_progress=False
    )

    two_column.change(update_doc_config, [two_column], None)

    # LLM Models
    keyTxt.change(current_model.value.set_key, keyTxt, [status_display]).then(**get_usage_args)
    keyTxt.submit(**get_usage_args)
    model_select_dropdown.change(current_model.value.get_model, [model_select_dropdown, lora_select_dropdown, keyTxt, temperature_slider, top_p_slider, systemPromptTxt], [status_display, lora_select_dropdown], show_progress=True)
    lora_select_dropdown.change(current_model.value.get_model, [model_select_dropdown, lora_select_dropdown, keyTxt, temperature_slider, top_p_slider, systemPromptTxt], [status_display], show_progress=True)

    # Template
    systemPromptTxt.change(current_model.value.set_system_prompt, [systemPromptTxt], None)
    templateRefreshBtn.click(get_template_names, None, [templateFileSelectDropdown])
    templateFileSelectDropdown.change(
        load_template,
        [templateFileSelectDropdown],
        [promptTemplates, templateSelectDropdown],
        show_progress=True,
    )
    templateSelectDropdown.change(
        get_template_content,
        [promptTemplates, templateSelectDropdown, systemPromptTxt],
        [systemPromptTxt],
        show_progress=True,
    )

    # S&L
    saveHistoryBtn.click(
        current_model.value.save_chat_history,
        [saveFileName, chatbot, user_name],
        downloadFile,
        show_progress=True,
    )
    saveHistoryBtn.click(get_history_names, [gr.State(False), user_name], [historyFileSelectDropdown])
    exportMarkdownBtn.click(
        current_model.value.export_markdown,
        [saveFileName, chatbot, user_name],
        downloadFile,
        show_progress=True,
    )
    historyRefreshBtn.click(get_history_names, [gr.State(False), user_name], [historyFileSelectDropdown])
    historyFileSelectDropdown.change(**load_history_from_file_args)
    downloadFile.change(**load_history_from_file_args)

    # Advanced
    max_context_length_slider.change(current_model.value.set_token_upper_limit, [max_context_length_slider], None)
    temperature_slider.change(current_model.value.set_temperature, [temperature_slider], None)
    top_p_slider.change(current_model.value.set_top_p, [top_p_slider], None)
    n_choices_slider.change(current_model.value.set_n_choices, [n_choices_slider], None)
    stop_sequence_txt.change(current_model.value.set_stop_sequence, [stop_sequence_txt], None)
    max_generation_slider.change(current_model.value.set_max_tokens, [max_generation_slider], None)
    presence_penalty_slider.change(current_model.value.set_presence_penalty, [presence_penalty_slider], None)
    frequency_penalty_slider.change(current_model.value.set_frequency_penalty, [frequency_penalty_slider], None)
    logit_bias_txt.change(current_model.value.set_logit_bias, [logit_bias_txt], None)
    user_identifier_txt.change(current_model.value.set_user_identifier, [user_identifier_txt], None)

    default_btn.click(
        reset_default, [], [apihostTxt, proxyTxt, status_display], show_progress=True
    )
    changeAPIURLBtn.click(
        change_api_host,
        [apihostTxt],
        [status_display],
        show_progress=True,
    )
    changeProxyBtn.click(
        change_proxy,
        [proxyTxt],
        [status_display],
        show_progress=True,
    )

logging.info(
    colorama.Back.GREEN
    + "\nå·è™çš„æ¸©é¦¨æç¤ºï¼šè®¿é—® http://localhost:7860 æŸ¥çœ‹ç•Œé¢"
    + colorama.Style.RESET_ALL
)
# é»˜è®¤å¼€å¯æœ¬åœ°æœåŠ¡å™¨ï¼Œé»˜è®¤å¯ä»¥ç›´æ¥ä»IPè®¿é—®ï¼Œé»˜è®¤ä¸åˆ›å»ºå…¬å¼€åˆ†äº«é“¾æ¥
demo.title = "å·è™ChatGPT ğŸš€"

if __name__ == "__main__":
    reload_javascript()
    # if running in Docker
    if dockerflag:
        if authflag:
            demo.queue(concurrency_count=CONCURRENT_COUNT).launch(
                server_name="0.0.0.0",
                server_port=7860,
                auth=auth_list,
                favicon_path="./assets/favicon.ico",
            )
        else:
            demo.queue(concurrency_count=CONCURRENT_COUNT).launch(
                server_name="0.0.0.0",
                server_port=7860,
                share=False,
                favicon_path="./assets/favicon.ico",
            )
    # if not running in Docker
    else:
        if authflag:
            demo.queue(concurrency_count=CONCURRENT_COUNT).launch(
                share=False,
                auth=auth_list,
                favicon_path="./assets/favicon.ico",
                inbrowser=True,
            )
        else:
            demo.queue(concurrency_count=CONCURRENT_COUNT).launch(
                share=False, favicon_path="./assets/favicon.ico", inbrowser=True
            )  # æ”¹ä¸º share=True å¯ä»¥åˆ›å»ºå…¬å¼€åˆ†äº«é“¾æ¥
        # demo.queue(concurrency_count=CONCURRENT_COUNT).launch(server_name="0.0.0.0", server_port=7860, share=False) # å¯è‡ªå®šä¹‰ç«¯å£
        # demo.queue(concurrency_count=CONCURRENT_COUNT).launch(server_name="0.0.0.0", server_port=7860,auth=("åœ¨è¿™é‡Œå¡«å†™ç”¨æˆ·å", "åœ¨è¿™é‡Œå¡«å†™å¯†ç ")) # å¯è®¾ç½®ç”¨æˆ·åä¸å¯†ç 
        # demo.queue(concurrency_count=CONCURRENT_COUNT).launch(auth=("åœ¨è¿™é‡Œå¡«å†™ç”¨æˆ·å", "åœ¨è¿™é‡Œå¡«å†™å¯†ç ")) # é€‚åˆNginxåå‘ä»£ç†
